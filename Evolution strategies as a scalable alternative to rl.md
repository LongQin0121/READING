
# Evolution Strategies as a Scalable Alternative to Reinforcement Learning
## OpenAI 团队
这篇文章的重点之一就是说明 ES 在计算速度上的优势，主要原因是：

高度可并行（Highly Parallelizable）

ES 每次迭代需要做的事情是：对策略加上很多随机扰动，然后分别在环境中跑一遍、算奖励。

这些扰动策略之间 互不依赖，所以可以同时分配到成百上千个 CPU/GPU 上去跑。

结果是：资源越多，速度几乎可以 线性提升。

不需要反向传播（No Backpropagation）

传统 RL（比如 DQN、PPO）需要反向传播来更新参数，涉及梯度计算和复杂的优化过程。

ES 只需要前向运行策略 → 得到奖励 → 聚合结果，不用算梯度，省掉了很多计算。

实现简单，通信开销小

并行节点之间只需要传递一个“适应度值”（奖励标量），而不是整条梯度信息或经验回放池，通信成本更低。

所以文章想表达的是：
👉 ES 的计算效率很高，特别是在大规模分布式环境里，它可以比传统 RL 更快